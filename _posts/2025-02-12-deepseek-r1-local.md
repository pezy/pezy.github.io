---
layout: post
title: "值得花精力折腾 DeepSeek R1 本地部署吗？"
tags: [deepseek-r1, local-deployment]
---

> <https://atlassc.net/2025/01/29/run-deepseek-r1>

最近很多人在折腾本地部署 DeepSeek R1，但实事求是的讲，有多少人真正具备合适的硬件条件？本地部署的"小"模型适合做哪些事情？

<!--more-->

这篇博客很详细的给出了一些硬件条件要求，相信看了之后，大家心里基本有个数了。我们真正惊叹，觉得有用的满血 DeepSeek R1 671B 模型，如果不是某个机构或者公司，我觉得基本是不太可能本地部署的。

剩下的就要看看蒸馏以及量化后的那些了。

# 首先明确一些概念

如何理解**蒸馏(Distilled)**？简单说，如果原始的大模型是教师，那么蒸馏后的就是学生。学生可以学到多少教师的知识呢？我们看模型名称后面备注的 xx b 就是用来标识这个的。

目前针对 DeepSeek R1，第一梯队的学生是 70B 的。从博客里的表格可以看到，mac 基本与此就无缘了（当然如果你用 exo 组集群另说）。最强的单机 MacOS，可以冲一冲 32b 的。

另一个概念是**量化（Quantized）**。这个其实就是浮点转定点的老问题，从 FPGA 时代就一直在做了。简单说，量化后，模型的精度会有损失。你可以理解成变得更加粗糙了。

在 huggingface 上的模型介绍页面，或者模型名称上，一般都会标识 Q xx，这个 xx 就是量化后的精度。譬如 Q4，指的就是 int4。

而当我们运行 `ollama run deepseek-r1` 的时候，实际上运行的是什么模型呢？从这里可以看到端倪:

> <https://ollama.com/library/deepseek-r1>

![image](https://github.com/user-attachments/assets/de454d80-cf86-4cde-9c6b-82d3376998be)

- `qwen2` 代表使用的是 Qwen2 的**架构**
- `7.62B` 代表模型的**蒸馏后**参数大小
- `Q4_K_M` 代表的就是**量化后**的精度了

`qwen2` 架构的好处一般是对中文的支持上。而从 `7b` 和 `q4` 上，你基本就能隐约感受到这个模型与满血版的差距了。

# 我想精确查询我的硬件条件能部署什么级别的模型

一般来说，我们关注以下几个硬件指标：

- RAM，内存
- VRAM，显存
- Context Window，上下文长度

那么可以从这个很方便的网站里查询:

> <https://www.canirunthisllm.net/stop-chart/>

拖动下面的滑动条，你可以很直观的看到，对于 70B 的完整精度版本(float32)，对于内存的要求已经超过 285.6 GB 了。是不是有种与我无瓜的感觉了呢？

# 本地部署后，我想共享给伙伴和同事，甚至我希望随时可以访问我的本地模型

`ollama serve` 默认只允许本地访问，如果要共享，需要 `export OLLAMA_HOST=0.0.0.0:11434` 后执行。这样你通过 `http://ip:11434` 就可以访问到 ollama 的 API 了。

但 ollama 官方是没有提供 API KEY 配置的，以至于很多伙伴的 ollama 就裸跑在公网上，譬如在 FOFA 上查看北京的：

> <https://en.fofa.info/result?qbase64=YXBwPSJPbGxhbWEiICYmIGlzX2RvbWFpbj1mYWxzZSAmJiBjb3VudHJ5PSJDTiIgJiYgY2l0eT0iQmVpamluZyI%3D>

不夸张，你可以免费享用许多人本地部署的成果。😂

已经有不少人在官方呼吁对鉴权的内置支持:

> <https://github.com/ollama/ollama/issues/849>

但官方似乎不为所动或者行动缓慢。🐶

如果你真有分享给多人的需求，可以考虑使用类似:

> <https://github.com/bartolli/ollama-bearer-auth-caddy>

这样的工具代理一下。

# 本地部署的模型怎么感觉与官方的差距如此之大？

没错，上面其实已经讲述了蒸馏与量化的概念，官方是一个博学多才的老师，你本地部署的不过是其一位较为粗心的学生。某些问题上，的确可能呈现出人与狗的差别来。

那么问题来了，本地部署还有什么意义？

事情都是有两面性的，博学多才的老师，可能年纪大了，步履蹒跚，你请他回答一些问题，他可能慢条斯理，而你理解起来也需要耗费时日。—— 就像读论文的感觉。
而那个抄作业的粗心学生，你看他的笔记，可以快速的了解大概。且他也反应敏捷（无知者无畏嘛），而且说起来滔滔不绝，量产如母猪。—— 就像看短视频的感觉。

所以取决于你用模型来做什么。本地的模型虽然粗笨了些，但对于一些繁琐却简单的任务，完全可以交给它：

- 文件分类？垃圾邮件判断？
- 文本处理，譬如提取某些关键信息
- 拼写、语法检查
- 物理常识的问答
- 粗粒度的摘要
- 简单多语言翻译

这就好比我们过去用代码实现的一些小功能函数，完全可以用它来代劳了。

综上，你觉得花功夫折腾本地部署值得不？
